{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562aa278-0e30-4c71-a827-2b1435d9eae0",
   "metadata": {},
   "source": [
    "# Create Daily Networks\n",
    "\n",
    "This notebook creates a Reddit user reply network for each day from Sep-Nov 2012.\n",
    "\n",
    "**You must download and unzip the September, November, and December comment files\n",
    "from https://files.pushshift.io/reddit/comments/ before running this notebook.\n",
    "You must name them and place them as follows:**\n",
    "- `./data/RC_2012-09`\n",
    "- `./data/RC_2012-10`\n",
    "- `./data/RC_2012-11`\n",
    "\n",
    "**You must manually create the following directories before running this notebook:**\n",
    "- `./data/RC_2012-09_daily/`\n",
    "- `./data/RC_2012-10_daily/`\n",
    "- `./data/RC_2012-11_daily/`\n",
    "- `./data/RC_2012-09_daily_graphs/`\n",
    "- `./data/RC_2012-10_daily_graphs/`\n",
    "- `./data/RC_2012-11_daily_graphs/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6fe933-64f8-4827-88ab-267f5b41785d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d3a8e1-1350-4f7d-9931-e68145bdf993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import psaw\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from pathlib import Path\n",
    "from calendar import Calendar\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c81898d-345d-45cc-bbf3-a8cc6255b934",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7041ad7b-4d6b-4799-a8f5-9b6f5f0d9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast group by subreddit\n",
    "# https://stackoverflow.com/questions/22219004/how-to-group-dataframe-rows-into-list-in-pandas-groupby\n",
    "#\n",
    "# expects df has two columns, first 'author,' then subreddit\n",
    "def group_subreddits_by_author(df):\n",
    "    keys, values = df.sort_values('author').values.T\n",
    "    ukeys, index = np.unique(keys, True)\n",
    "    arrays = np.split(values, index[1:]) # subreddit must be 2nd col\n",
    "    return pd.DataFrame({\n",
    "        'author': ukeys,\n",
    "        'subreddits': [set(a) for a in arrays]\n",
    "    })\n",
    "\n",
    "\n",
    "# expects df has two columns, first 'author,' then subreddit\n",
    "def build_subreddit_shared_author_graph(df):\n",
    "    grouped_by_sub = group_subreddits_by_author(df)\n",
    "    G = nx.Graph()\n",
    "    for shared_subs in grouped_by_sub['subreddits']:\n",
    "        for sub1, sub2 in itertools.combinations(shared_subs, 2):\n",
    "            if G.has_edge(sub1, sub2):\n",
    "                G[sub1][sub2]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(sub1, sub2, weight=1)\n",
    "    return G\n",
    "\n",
    "\n",
    "def export_to_gephi_file(G, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for line in nx.generate_gexf(G):\n",
    "            f.write(line + '\\n')\n",
    "            \n",
    "\n",
    "def label_users(df, pol_subs):\n",
    "    gp_by_sub = group_subreddits_by_author(df[['author', 'subreddit']])\n",
    "    for sub in pol_subs:\n",
    "        other_subs = {s for s in pol_subs if s != sub}\n",
    "        for i, row in gp_by_sub.iterrows():\n",
    "            ss = row['subreddits']\n",
    "            if sub in ss and ss.isdisjoint(other_subs):\n",
    "                gp_by_sub.at[i, 'political_label'] = sub\n",
    "    return gp_by_sub\n",
    "\n",
    "\n",
    "def load_comments_from_files(file_paths):\n",
    "    dfs = []\n",
    "    for file_path in file_paths:\n",
    "        dfs.append(load_comments_from_file(file_path))\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "\n",
    "def load_comments_from_file(file_path, limit=None):\n",
    "    keys_to_keep = ['author', 'subreddit', 'score', 'controversiality', 'created_utc', 'id', 'parent_id', 'body']\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        count = 0\n",
    "        for line in tqdm(f):\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                record = { k: j[k] for k in keys_to_keep }\n",
    "                data.append(record)\n",
    "                count += 1\n",
    "                if limit and count > limit:\n",
    "                    break\n",
    "            except json.JSONDecodeError:\n",
    "                break\n",
    "    df = pd.json_normalize(data)\n",
    "    df[['score', 'controversiality', 'created_utc']] = df[['score', 'controversiality', 'created_utc']].apply(pd.to_numeric, downcast=\"float\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# function to remove punctuation from text (input is a string)\n",
    "def clean_text(s):\n",
    "\treturn \"\".join(l for l in str(s) if l not in string.punctuation)\n",
    "    \n",
    "    \n",
    "def get_sub_comments(pol_users, comments, subs):\n",
    "    sub_users = {sub: pol_users[pol_users['political_label'] == sub].index for sub in subs}\n",
    "    pcids = {sub: [] for sub in subs}\n",
    "    for comment in tqdm(comments.itertuples()):\n",
    "        for sub in subs:\n",
    "            if getattr(comment, 'author') in sub_users[sub] and getattr(comment, 'subreddit') == sub:\n",
    "                pcids[sub].append(getattr(comment, 'Index'))\n",
    "    return pcids\n",
    "\n",
    "\n",
    "def save_comments_by_day(src, dest, month, year = 2012):\n",
    "    num_days = calendar.monthrange(year, month)[1]\n",
    "    day_starts = []\n",
    "    day_files = []\n",
    "    day_writers = []\n",
    "    keys_to_keep = ['author', 'subreddit', 'score', 'controversiality', 'created_utc', 'id', 'parent_id', 'body']\n",
    "     \n",
    "    for day in range(1, num_days + 1):\n",
    "        start_of_day = int(dt.datetime(year, month, day, tzinfo=dt.timezone.utc).timestamp())\n",
    "        print(start_of_day)\n",
    "        day_starts.append(start_of_day)\n",
    "        day_file = open(f'{dest}{day}.csv', 'w', newline='')\n",
    "        day_files.append(day_file)\n",
    "        day_writer = csv.writer(day_file, delimiter=',', quotechar='\"')\n",
    "        day_writer.writerow(keys_to_keep)\n",
    "        day_writers.append(day_writer)\n",
    "    \n",
    "    with open(src, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            try:\n",
    "                j = json.loads(line)\n",
    "                j['created_utc'] = int(j['created_utc'])\n",
    "                i = len(day_starts) - 1\n",
    "                while (j['created_utc'] < day_starts[i]):\n",
    "                    i = i - 1\n",
    "                record = [ j[k] for k in keys_to_keep ]\n",
    "                day_writers[i].writerow(record)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    for day_file in day_files:\n",
    "        day_file.close()\n",
    "        \n",
    "        \n",
    "def get_bow_models(comments, sub_comments):\n",
    "    models = dict()\n",
    "    for sub, cids in sub_comments.items():\n",
    "        corpus = comments.loc[cids]['body'].apply(clean_text)\n",
    "        model = TfidfVectorizer()\n",
    "        model.fit(corpus)\n",
    "        models[sub] = model\n",
    "    return models\n",
    "\n",
    "\n",
    "def load_comments(month):\n",
    "    comments = pd.DataFrame()\n",
    "    for i in tqdm(range(1, 32)):\n",
    "        try:\n",
    "            comments = comments.append(pd.read_csv(f'./data/RC_2012-{month}_daily/{i}.csv'))\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    comments.set_index('id', inplace=True)\n",
    "    return comments\n",
    "\n",
    "\n",
    "def get_pu_label(user):\n",
    "        try:\n",
    "            return pol_users.at[user, 'political_label']\n",
    "        except KeyError:\n",
    "            return None\n",
    "    \n",
    "\n",
    "def analyze_users(month, comments=None):\n",
    "    if comments is None:\n",
    "        comments = load_comments(month)\n",
    "    \n",
    "    pol_subs = ['Conservative', 'Liberal']\n",
    "    pol_users = label_users(comments, set(pol_subs))\n",
    "    pol_users.set_index('author', inplace=True)\n",
    "    \n",
    "    sub_comments = get_sub_comments(pol_users, comments, pol_subs)\n",
    "    bow_models = get_bow_models(comments, sub_comments)\n",
    "    \n",
    "    sub_limit = 5\n",
    "    subs_of_interest = pd.read_csv('./subreddits_of_interest.csv')\n",
    "    subs_of_interest = set(subs_of_interest.sort_values('submission_amount', ascending=False)['subreddit'].head(sub_limit))\n",
    "    subs_of_interest = subs_of_interest | set(pol_subs)\n",
    "    \n",
    "    pol_users['political_label'] = pol_users['political_label'].fillna('unaffiliated')\n",
    "    pol_users.to_csv(f'users_{month}.csv')\n",
    "\n",
    "    power_users = pd.read_csv('power_users.csv')\n",
    "    power_users['label'] = power_users['user'].apply(get_pu_label)\n",
    "    power_users = power_users[power_users['label'].notnull()]\n",
    "    power_users.to_csv(f'power_users_{month}.csv')\n",
    "    \n",
    "    return (pol_users, bow_models, subs_of_interest)\n",
    "\n",
    "\n",
    "def build_political_user_reply_graph(comments, users, bow_models, subs_of_interest):\n",
    "    pol_users = dict()\n",
    "    for i, user in users.iterrows():\n",
    "        if not user['political_label'] == 'unaffiliated':\n",
    "            pol_users[i] = user['political_label']\n",
    "    G = nx.DiGraph()\n",
    "    for comment in comments.itertuples():\n",
    "        i = getattr(comment, 'Index')\n",
    "        user1 = getattr(comment, 'author')\n",
    "        csub = getattr(comment, 'subreddit')\n",
    "        if csub not in subs_of_interest:\n",
    "            continue\n",
    "        typed_parent_id = getattr(comment, 'parent_id')\n",
    "        if not typed_parent_id.startswith('t1_'): # Comment\n",
    "            continue\n",
    "        parent_id = typed_parent_id[3:]\n",
    "        if parent_id not in comments.index:\n",
    "            continue\n",
    "        parent = comments.loc[parent_id]\n",
    "        user2 = parent['author']\n",
    "        \n",
    "        for sub, model in bow_models.items():\n",
    "            body = getattr(comment, 'body')\n",
    "            clean = clean_text(body)\n",
    "\n",
    "            parent_body = parent['body']\n",
    "            parent_clean = clean_text(parent_body)\n",
    "\n",
    "            bows = model.transform([clean, parent_clean]).toarray() # TODO: operate on sparse version (no toarray)?\n",
    "            bow = bows[0]\n",
    "            parent_bow = bows[1]\n",
    "\n",
    "            similarity = cosine_similarity(bows)[1][0]\n",
    "            pol_score = sum(bow) / len(bow)\n",
    "            \n",
    "            sim_key = f'{sub}_sim'\n",
    "            pol_key = f'{sub}_pol'\n",
    "            if G.has_edge(user1, user2):\n",
    "                if sim_key in G[user1][user2]:\n",
    "                    G[user1][user2][sim_key].append(similarity)\n",
    "                else:\n",
    "                    G[user1][user2][sim_key] = [similarity]\n",
    "                if pol_key in G[user1][user2]:                    \n",
    "                    G[user1][user2][pol_key].append(pol_score)\n",
    "                else:\n",
    "                    G[user1][user2][pol_key] = [pol_score]\n",
    "            else:\n",
    "                G.add_edge(user1, user2, weight=0, subreddits=set(), **{f'{sub}_sim': [similarity], f'{sub}_pol': [pol_score]})\n",
    "            \n",
    "        if G.has_edge(user1, user2):\n",
    "            G[user1][user2]['weight'] += 1\n",
    "            G[user1][user2]['subreddits'].add(csub)\n",
    "        else:\n",
    "            G.add_edge(user1, user2, weight=1, subreddits=set([csub]))\n",
    "            \n",
    "    for node in G.nodes:\n",
    "        G.nodes[node]['political_label'] = users.at[node, 'political_label']\n",
    "    for a, b in G.edges:\n",
    "        G[a][b]['subreddits'] = \",\".join(G[a][b]['subreddits'])\n",
    "        for sub in bow_models.keys():\n",
    "            G[a][b][f'avg_{sub}_sim'] = np.average(G[a][b][f'{sub}_sim'])\n",
    "            G[a][b][f'avg_{sub}_pol'] = np.average(G[a][b][f'{sub}_pol'])\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d854f65c-b07a-4ba2-942c-088042351bec",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "\n",
    "This cell splits the Pushshift comment data files for Sep, Oct, and Nov into daily comment data files so we can save memory by loading one day at a time when creating the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ad6960-1cf6-4c6c-9cf8-ae67e9d313b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must manually create the three \"_daily\" directories before running this cell\n",
    "for comment_file_name, dest, month in [\n",
    "    ('./data/RC_2012-09', './data/RC_2012-09_daily/', 9),\n",
    "    ('./data/RC_2012-10', './data/RC_2012-10_daily/', 10),\n",
    "    ('./data/RC_2012-11', './data/RC_2012-11_daily/', 11)\n",
    "]:\n",
    "    save_comments_by_day(comment_file_name, dest, month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429cf48c-db5a-4937-a341-efe05b52926c",
   "metadata": {},
   "source": [
    "## Create Networks\n",
    "\n",
    "For each month, this cell analyzes the users and assigns them political labels, then creates a bag-of-words model for each labeled group. With these models, it creates user reply networks for each day which include data about the political language used in the replies, as measured by the bag-of-words models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5945441e-9067-410c-8185-acca57ae6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must manually create the three \"_daily_graphs\" directories before running this cell\n",
    "for month in ['09', '10', '11']:\n",
    "    pol_users, bow_models, subs_of_interest = analyze_users(month)\n",
    "    for i in tqdm(range(1, 32)):\n",
    "        try:\n",
    "            comments = pd.read_csv(f'./data/RC_2012-{month}_daily/{i}.csv')\n",
    "            comments.set_index('id', inplace=True)\n",
    "            G = build_political_user_reply_graph(comments, pol_users, bow_models, subs_of_interest)\n",
    "            for a, b in G.edges:\n",
    "                for sub in bow_models.keys():\n",
    "                    del G[a][b][f'{sub}_sim']\n",
    "                    del G[a][b][f'{sub}_pol']\n",
    "            nx.readwrite.gexf.write_gexf(G, f'./data/RC_2012-{month}_daily_graphs/{i}.gexf')\n",
    "        except FileNotFoundError:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
